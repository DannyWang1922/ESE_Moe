model_name_or_path: google-bert/bert-base-uncased
task: "stanfordnlp/sst2"
batch_size: 16
max_length: 512
seed: 42
output_dir: "./results/bert_moe_sst2"
epochs: 10
earning_rate: 5e-5
weight_decay: 0.01

warmup_steps: 500
logging_dir: "./logs"
logging_steps: 100
eval_steps: 500
save_steps: 500
fp16: False

num_experts: 8
top_k: 2
expert_dropout: 0.1
router_temperature: 0.1
router_noise_epsilon: 1e-2
router_training_noise: true
use_load_balancing: true
router_z_loss_coef: 1e-3
router_aux_loss_coef: 0.01
track_expert_metrics: true
expert_init_strategy: "identical"
load_balance_loss_weight: 0.01
moe_layers: "[11]"